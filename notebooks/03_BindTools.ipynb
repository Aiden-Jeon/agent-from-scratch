{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "env_file = \"../.env\"\n",
    "if os.path.exists(env_file):\n",
    "    load_dotenv()\n",
    "else:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"<YOUR-OPENAI-API-KEY>\"\n",
    "    os.environ[\"HF_TOKEN\"] = \"<YOUR-HUGGINGFACE-API-KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I am a digital AI assistant model designed to assist with answering questions and providing information.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatLiteLLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "# openai_llm = ChatLiteLLM(model=\"openai/gpt-3.5-turbo\")\n",
    "openai_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "messages = [HumanMessage(content=\"what model are you\")]\n",
    "openai_llm.invoke(messages).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Iâ€™m a large language model. When you ask me a question or provide me with a prompt, I analyze what you say and generate a response that is relevant and accurate. I'm constantly learning and improving, so over time I'll be even better at assisting you. Is there anything I can help you with?\n"
     ]
    }
   ],
   "source": [
    "hug_llm = ChatLiteLLM(\n",
    "    model=\"huggingface/meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    temperature=0.2,\n",
    ")\n",
    "messages = [HumanMessage(content=\"what model are you\")]\n",
    "hug_llm.invoke(messages).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "In this tutorial we will see how to select functions with OpenAI function call and call it on local.\n",
    "We will use the [langchain tutorial](https://python.langchain.com/docs/how_to/tool_calling/) code for this tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions with type hint and description.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function name, type hints, and docstring are all part of the tool\n",
    "# schema that's passed to the model. Defining good, descriptive schemas\n",
    "# is an extension of prompt engineering and is an important part of\n",
    "# getting models to perform well.\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bind the functions with `bind_tools` attribute.\n",
    "After that when we invoke with this chain, llm will respond to call the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_Dpk43sJgWHnJM4pdotVJmRql)\n",
      " Call ID: call_Dpk43sJgWHnJM4pdotVJmRql\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 12\n"
     ]
    }
   ],
   "source": [
    "tools = [add, multiply]\n",
    "llm_with_tools = openai_llm.bind_tools(tools)\n",
    "\n",
    "response = llm_with_tools.invoke(\"What is 3 * 12?\")\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returned response's has `tool_calls` attribute.\n",
    "This tools mean that llm ask to client to run those tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 3, 'b': 12},\n",
       "  'id': 'call_XMNaOeroWt0MiVCxCpwBfmGc',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tool calls can be multiple.\n",
    "This is example query to get multiple tool calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_WhdJSIWSafayJRpxeedZQ5yW)\n",
      " Call ID: call_WhdJSIWSafayJRpxeedZQ5yW\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 12\n",
      "  add (call_ohGu3oiMauivYgZy5pesoDkr)\n",
      " Call ID: call_ohGu3oiMauivYgZy5pesoDkr\n",
      "  Args:\n",
      "    a: 11\n",
      "    b: 49\n"
     ]
    }
   ],
   "source": [
    "response = llm_with_tools.invoke(\"What is 3 * 12? Also, what is 11 + 49?\")\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 3, 'b': 12},\n",
       "  'id': 'call_KPjyAraQtDTMTIb5C8MhZbC4',\n",
       "  'type': 'tool_call'},\n",
       " {'name': 'add',\n",
       "  'args': {'a': 11, 'b': 49},\n",
       "  'id': 'call_luj7ScO92RIyfOh2Id1gYmSg',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is not valid tools to call it will respond that there is not valid one.\n",
    "And `tool_calls` will be empty list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, could you please provide more context or specify what you would like to know or calculate?\n"
     ]
    }
   ],
   "source": [
    "query = \"What is?\"\n",
    "\n",
    "response = llm_with_tools.invoke(query)\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI\n",
    "\n",
    "What happen when we use `bind_tools`?\n",
    "Before tutorial, We already check how the tool is calling with structured output.\n",
    "This function call is basically same, but only we have convert tools as json format like this:\n",
    "\n",
    "```python\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\": \"string\"},\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"c\", \"f\"]},\n",
    "                },\n",
    "                \"required\": [\"location\", \"unit\"],\n",
    "                \"additionalProperties\": False,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "And langchain helps us to make this json type easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'add',\n",
       "   'description': 'Add two integers.',\n",
       "   'parameters': {'properties': {'a': {'description': 'First integer',\n",
       "      'type': 'integer'},\n",
       "     'b': {'description': 'Second integer', 'type': 'integer'}},\n",
       "    'required': ['a', 'b'],\n",
       "    'type': 'object'}}},\n",
       " {'type': 'function',\n",
       "  'function': {'name': 'multiply',\n",
       "   'description': 'Multiply two integers.',\n",
       "   'parameters': {'properties': {'a': {'description': 'First integer',\n",
       "      'type': 'integer'},\n",
       "     'b': {'description': 'Second integer', 'type': 'integer'}},\n",
       "    'required': ['a', 'b'],\n",
       "    'type': 'object'}}}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "\n",
    "openai_tools = [convert_to_openai_tool(tool) for tool in tools]\n",
    "openai_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_HuktrZrtKxIXNZk6QOAPjbpJ', function=Function(arguments='{\"a\":3,\"b\":12}', name='multiply'), type='function')])\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is 3 * 12?\",\n",
    "        },\n",
    "    ],\n",
    "    tools=openai_tools,\n",
    ")\n",
    "event = completion.choices[0].message\n",
    "print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessageToolCall(id='call_HuktrZrtKxIXNZk6QOAPjbpJ', function=Function(arguments='{\"a\":3,\"b\":12}', name='multiply'), type='function')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='It seems like your question is incomplete or unclear. Could you please provide more context or specify what you would like to know?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is?\",\n",
    "        },\n",
    "    ],\n",
    "    tools=openai_tools,\n",
    ")\n",
    "event = completion.choices[0].message\n",
    "print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface\n",
    "\n",
    "Now we will make this process manual for the unsupported llm endpoint.\n",
    "\n",
    "To make this we will use `langchain.tools`.\n",
    "Just decorating the function we can get format that can use in prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This converted tool has attributes like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add\n",
      "Add two integers.\n",
      "\n",
      "    Args:\n",
      "        a: First integer\n",
      "        b: Second integer\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n",
      "multiply\n",
      "Multiply two integers.\n",
      "\n",
      "    Args:\n",
      "        a: First integer\n",
      "        b: Second integer\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "langchain_tools = [add, multiply]\n",
    "\n",
    "for tool in langchain_tools:\n",
    "    print(tool.name)\n",
    "    print(tool.description)\n",
    "    print(tool.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render tools to use in prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add(a: int, b: int) -> int - Add two integers.\n",
      "\n",
      "    Args:\n",
      "        a: First integer\n",
      "        b: Second integer\n",
      "multiply(a: int, b: int) -> int - Multiply two integers.\n",
      "\n",
      "    Args:\n",
      "        a: First integer\n",
      "        b: Second integer\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import render_text_description\n",
    "\n",
    "\n",
    "rendered_tools = render_text_description(langchain_tools)\n",
    "tool_names = [tool.name for tool in langchain_tools]\n",
    "print(rendered_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is prompt that we will use.\n",
    "\n",
    "Prompt generates a json format with key, name and args.\n",
    "`name` key is name of the tool we will use, but if there is not proper tool to call it will generate refusal.\n",
    "We will use this refusal later, to handle edge case.\n",
    "And `args` are dict type that contains values which is used to invoke tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# https://smith.langchain.com/hub/hwchase17/react-chat-json\n",
    "system_prompt = \"\"\"\\\n",
    "TOOLS\n",
    "------\n",
    "Assistant can ask the user to use tools to look up information that may be helpful in answering the users original question.\n",
    "The tools the human can use are:\n",
    "\n",
    "{rendered_tools}\n",
    "\n",
    "RESPONSE FORMAT INSTRUCTIONS\n",
    "----------------------------\n",
    "\n",
    "When responding to me, please output a response in one of three formats:\n",
    "\n",
    "**Option 1:**\n",
    "Use this if you cannot determine which tool to use.\n",
    "Markdown code snippet formatted in the following schema:\n",
    "\n",
    "{{\n",
    "    \"name\": \"refusal\"\n",
    "}}\n",
    "\n",
    "\n",
    "**Option 2:**\n",
    "Use this if you want the human to use a tool.\n",
    "Markdown code snippet formatted in the following schema:\n",
    "\n",
    "{{\n",
    "    \"name\": string, \\\\ The action to take. Must be one of {tool_names}\n",
    "    \"args\": dict, \\\\ The input to the action described {rendered_tools}\n",
    "}}\n",
    "\n",
    "USER'S INPUT\n",
    "--------------------\n",
    "\n",
    "Here is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt), (\"user\", \"{input}\")]\n",
    ")\n",
    "prompt = prompt.partial(\n",
    "    rendered_tools=rendered_tools,\n",
    "    tool_names=tool_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will return with json type.\n",
    "So we can easily parse this with `JsonOutputParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "{\n",
      "    \"name\": \"multiply\",\n",
      "    \"args\": {\n",
      "        \"a\": 3,\n",
      "        \"b\": 12\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | hug_llm\n",
    "response = chain.invoke({\"What is 3 * 12?\"})\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'multiply', 'args': {'a': 3, 'b': 12}}"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "chain = prompt | hug_llm | JsonOutputParser()\n",
    "response = chain.invoke({\"What is 3 * 12?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With [langchain tutorial](https://python.langchain.com/docs/how_to/tools_prompting/), we will define a function `invoke_tool` to call the proper tool and give its output to chain result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional, TypedDict\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "class ToolCallRequest(TypedDict):\n",
    "    \"\"\"A typed dict that shows the inputs into the invoke_tool function.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    arguments: Dict[str, Any]\n",
    "\n",
    "\n",
    "def invoke_tool(\n",
    "    tool_call_request: ToolCallRequest, config: Optional[RunnableConfig] = None\n",
    "):\n",
    "    \"\"\"A function that we can use the perform a tool invocation.\n",
    "\n",
    "    Args:\n",
    "        tool_call_request: a dict that contains the keys name and arguments.\n",
    "            The name must match the name of a tool that exists.\n",
    "            The arguments are the arguments to that tool.\n",
    "        config: This is configuration information that LangChain uses that contains\n",
    "            things like callbacks, metadata, etc.See LCEL documentation about RunnableConfig.\n",
    "\n",
    "    Returns:\n",
    "        output from the requested tool\n",
    "    \"\"\"\n",
    "    if tool_call_request[\"name\"] == \"refusal\":\n",
    "        return None\n",
    "    tool_name_to_tool = {tool.name: tool for tool in langchain_tools}\n",
    "    name = tool_call_request[\"name\"]\n",
    "    requested_tool = tool_name_to_tool[name]\n",
    "    return requested_tool.invoke(tool_call_request[\"args\"], config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function can use like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke_tool({\"name\": \"multiply\", \"args\": {\"a\": 3, \"b\": 12}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make a chain to use `invoke_tool` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | hug_llm | JsonOutputParser() | invoke_tool\n",
    "chain.invoke({\"input\": \"What is 3 * 12?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this as a chain we can use `RunnablePassthrough` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'output': 36}"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    prompt\n",
    "    | hug_llm\n",
    "    | JsonOutputParser()\n",
    "    | RunnablePassthrough.assign(output=invoke_tool)\n",
    ")\n",
    "chain.invoke({\"input\": \"What is 3 * 12?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this `RunnablePassthrough` only get dictionary type, so we cannot pass over multiple tool calling.\n",
    "\n",
    "And this is why functions like `JsonOutputToolsParser` has an argument like `first_tool_only`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'output': 36}"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is 3 * 12? Also, what is 11 + 49?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can see that our prompt is using only first tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'add', 'args': {'a': 11, 'b': 49}, 'output': 60}"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is 11 + 49? Also, what is 3 * 12?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a case with error handling like not proper tool to call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'refusal', 'output': None}"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Tools\n",
    "\n",
    "Now we will fix prompt to support multiple tool calling.\n",
    "Add `tool_calls` key which contains a list of json schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# https://smith.langchain.com/hub/hwchase17/react-chat-json\n",
    "system_prompt = \"\"\"\\\n",
    "TOOLS\n",
    "------\n",
    "Assistant can ask the user to use tools to look up information that may be helpful in answering the users original question.\n",
    "The tools the human can use are:\n",
    "\n",
    "{rendered_tools}\n",
    "\n",
    "RESPONSE FORMAT INSTRUCTIONS\n",
    "----------------------------\n",
    "\n",
    "When responding to me, please output a response in one of three formats:\n",
    "\n",
    "**Option 1:**\n",
    "Use this if you cannot determine which tool to use.\n",
    "Markdown code snippet formatted in the following schema:\n",
    "\n",
    "{{\n",
    "    tool_calls:[\n",
    "        {{\n",
    "            \"name\": \"refusal\"\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "\n",
    "**Option 2:**\n",
    "Use this if you want the human to use a tool.\n",
    "Markdown code snippet formatted in the following schema:\n",
    "\n",
    "{{\n",
    "    tool_calls:[\n",
    "        {{\n",
    "            \"name\": string, \\\\ The action to take. Must be one of {tool_names}\n",
    "            \"args\": dict, \\\\ The input to the action described {rendered_tools}\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "**Option 3:**\n",
    "Use this if you want the human to use multiple tools.\n",
    "Markdown code snippet formatted in the following schema:\n",
    "\n",
    "{{\n",
    "    tool_calls:[\n",
    "        {{\n",
    "            \"name\": string, \\\\ The action to take. Must be one of {tool_names}\n",
    "            \"args\": dict, \\\\ The input to the action described {rendered_tools}\n",
    "        }},\n",
    "        {{\n",
    "            \"name\": string, \\\\ The action to take. Must be one of {tool_names}\n",
    "            \"args\": dict, \\\\ The input to the action described {rendered_tools}\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "USER'S INPUT\n",
    "--------------------\n",
    "\n",
    "Here is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt), (\"user\", \"{input}\")]\n",
    ")\n",
    "prompt = prompt.partial(\n",
    "    rendered_tools=rendered_tools,\n",
    "    tool_names=tool_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it will return like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_calls': [{'name': 'multiply', 'args': {'a': 3, 'b': 12}}]}"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "chain = prompt | hug_llm | JsonOutputParser()\n",
    "response = chain.invoke({\"What is 3 * 12?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this we have to fix the schema.\n",
    "\n",
    "First write `ToolCall` which is same as before example.\n",
    "And write `ToolCallRequests` which contains list of `ToolCall`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, TypedDict\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "class ToolCall(TypedDict):\n",
    "    \"\"\"A typed dict that shows the inputs into the invoke_tool function.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    arguments: Optional[Dict[str, Any]]\n",
    "\n",
    "\n",
    "class ToolCallRequests(TypedDict):\n",
    "    \"\"\"A typed dict that shows the inputs into the invoke_tool function.\"\"\"\n",
    "\n",
    "    tool_calls: List[ToolCall]\n",
    "\n",
    "\n",
    "def invoke_tools(\n",
    "    tool_call_requests: ToolCallRequests, config: Optional[RunnableConfig] = None\n",
    "):\n",
    "    \"\"\"A function that we can use the perform a tool invocation.\n",
    "\n",
    "    Args:\n",
    "        tool_call_request: a dict that contains the keys name and arguments.\n",
    "            The name must match the name of a tool that exists.\n",
    "            The arguments are the arguments to that tool.\n",
    "        config: This is configuration information that LangChain uses that contains\n",
    "            things like callbacks, metadata, etc.See LCEL documentation about RunnableConfig.\n",
    "\n",
    "    Returns:\n",
    "        output from the requested tool\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for tool_call_request in tool_call_requests[\"tool_calls\"]:\n",
    "        if tool_call_request[\"name\"] == \"refusal\":\n",
    "            return None\n",
    "        \n",
    "        tool_name_to_tool = {tool.name: tool for tool in langchain_tools}\n",
    "        name = tool_call_request[\"name\"]\n",
    "        requested_tool = tool_name_to_tool[name]\n",
    "        response = requested_tool.invoke(tool_call_request[\"args\"], config=config)\n",
    "        results += [response]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use `RunnablePassthrough` function.\n",
    "You can see that output has been changed as list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_calls': [{'name': 'multiply', 'args': {'a': 3, 'b': 12}}],\n",
       " 'output': [36]}"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    prompt\n",
    "    | hug_llm\n",
    "    | JsonOutputParser()\n",
    "    | RunnablePassthrough.assign(output=invoke_tools)\n",
    ")\n",
    "chain.invoke({\"input\": \"What is 3 * 12?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can handle multiple tool calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_calls': [{'name': 'multiply', 'args': {'a': 3, 'b': 12}},\n",
       "  {'name': 'add', 'args': {'a': 11, 'b': 49}}],\n",
       " 'output': [36, 60]}"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is 3 * 12? Also, what is 11 + 49?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also order is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_calls': [{'name': 'add', 'args': {'a': 11, 'b': 49}},\n",
       "  {'name': 'multiply', 'args': {'a': 3, 'b': 12}}],\n",
       " 'output': [60, 36]}"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is 11 + 49? Also, what is 3 * 12?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a case with error handling like not proper tool to call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_calls': [{'name': 'refusal'}], 'output': None}"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-from-scratch-3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
