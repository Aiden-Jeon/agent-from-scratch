{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bind Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q langchain langchain-openai langchain-community python-dotenv litellm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Setting\n",
    "\n",
    "Get tokens from the pages:\n",
    "- `OPENAI_API_KEY`: [OpenAI](https://platform.openai.com/settings/organization/api-keys)\n",
    "- `HF_TOKEN`: [Huggingface](https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "env_file = \".env\"\n",
    "if os.path.exists(env_file):\n",
    "    load_dotenv()\n",
    "else:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"<YOUR-OPENAI-API-KEY>\"\n",
    "    os.environ[\"HF_TOKEN\"] = \"<YOUR-HUGGINGFACE-API-KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I am a virtual assistant, powered by artificial intelligence.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatLiteLLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "# openai_llm = ChatLiteLLM(model=\"openai/gpt-3.5-turbo\")\n",
    "openai_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "messages = [HumanMessage(content=\"what model are you\")]\n",
    "openai_llm.invoke(messages).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I am Phi, created by Microsoft. I'm not powered by a traditional model with hardware components but rather I'm a software-based AI language model. My architecture is based on Microsoft's research and is designed to understand and generate human-like text based on the prompts I receive.\n"
     ]
    }
   ],
   "source": [
    "hug_llm = ChatLiteLLM(\n",
    "    model=\"huggingface/microsoft/Phi-3.5-mini-instruct\",\n",
    "    temperature=0.2,\n",
    ")\n",
    "messages = [HumanMessage(content=\"what model are you\")]\n",
    "hug_llm.invoke(messages).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "In this tutorial we will see how to select functions with OpenAI function call and call it on local.\n",
    "We will use the [langchain tutorial](https://python.langchain.com/docs/how_to/tool_calling/) code for this tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions with type hint and description.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function name, type hints, and docstring are all part of the tool\n",
    "# schema that's passed to the model. Defining good, descriptive schemas\n",
    "# is an extension of prompt engineering and is an important part of\n",
    "# getting models to perform well.\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bind the defined functions with `bind_tools` attribute on llm.\n",
    "When we ask query with this bind llm, llm will respond which function to call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_3YovBgEqlfpYIi8pm2HHRZuy)\n",
      " Call ID: call_3YovBgEqlfpYIi8pm2HHRZuy\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 12\n"
     ]
    }
   ],
   "source": [
    "tools = [add, multiply]\n",
    "llm_with_tools = openai_llm.bind_tools(tools)\n",
    "\n",
    "response = llm_with_tools.invoke(\"What is 3 * 12?\")\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returned response's has `tool_calls` attribute.\n",
    "This tools mean that llm ask to client to run those tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 3, 'b': 12},\n",
       "  'id': 'call_3YovBgEqlfpYIi8pm2HHRZuy',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tool calls can be multiple.\n",
    "This is example query to get multiple tool calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_FwR3ip6Bim8wbFvwCoXKmzng)\n",
      " Call ID: call_FwR3ip6Bim8wbFvwCoXKmzng\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 12\n",
      "  add (call_Wj4FWC26qkJAXgrzalF7NxQj)\n",
      " Call ID: call_Wj4FWC26qkJAXgrzalF7NxQj\n",
      "  Args:\n",
      "    a: 11\n",
      "    b: 49\n"
     ]
    }
   ],
   "source": [
    "response = llm_with_tools.invoke(\"What is 3 * 12? Also, what is 11 + 49?\")\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 3, 'b': 12},\n",
       "  'id': 'call_FwR3ip6Bim8wbFvwCoXKmzng',\n",
       "  'type': 'tool_call'},\n",
       " {'name': 'add',\n",
       "  'args': {'a': 11, 'b': 49},\n",
       "  'id': 'call_Wj4FWC26qkJAXgrzalF7NxQj',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is not valid tools to call it will respond that there is not valid one.\n",
    "And `tool_calls` will be empty list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "It seems like you haven't provided any specific question or task. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "query = \"What is?\"\n",
    "\n",
    "response = llm_with_tools.invoke(query)\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the selected tool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI\n",
    "\n",
    "What happen when we use `bind_tools`?\n",
    "Before tutorial, We already check how the tool is calling with structured output.\n",
    "This function call is basically same, but only we have convert tools as json format like this:\n",
    "\n",
    "```python\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\": \"string\"},\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"c\", \"f\"]},\n",
    "                },\n",
    "                \"required\": [\"location\", \"unit\"],\n",
    "                \"additionalProperties\": False,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "And langchain helps us to make this json type easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'add',\n",
       "   'description': 'Add two integers.',\n",
       "   'parameters': {'properties': {'a': {'description': 'First integer',\n",
       "      'type': 'integer'},\n",
       "     'b': {'description': 'Second integer', 'type': 'integer'}},\n",
       "    'required': ['a', 'b'],\n",
       "    'type': 'object'}}},\n",
       " {'type': 'function',\n",
       "  'function': {'name': 'multiply',\n",
       "   'description': 'Multiply two integers.',\n",
       "   'parameters': {'properties': {'a': {'description': 'First integer',\n",
       "      'type': 'integer'},\n",
       "     'b': {'description': 'Second integer', 'type': 'integer'}},\n",
       "    'required': ['a', 'b'],\n",
       "    'type': 'object'}}}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "\n",
    "openai_tools = [convert_to_openai_tool(tool) for tool in tools]\n",
    "openai_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_P6Kf9IYrgEo6qN3eszpAy6HQ', function=Function(arguments='{\"a\":3,\"b\":12}', name='multiply'), type='function')])\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is 3 * 12?\",\n",
    "        },\n",
    "    ],\n",
    "    tools=openai_tools,\n",
    ")\n",
    "event = completion.choices[0].message\n",
    "print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessageToolCall(id='call_P6Kf9IYrgEo6qN3eszpAy6HQ', function=Function(arguments='{\"a\":3,\"b\":12}', name='multiply'), type='function')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='It seems that your question is incomplete. Could you please provide more context or clarify what you would like to know?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is?\",\n",
    "        },\n",
    "    ],\n",
    "    tools=openai_tools,\n",
    ")\n",
    "event = completion.choices[0].message\n",
    "print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface\n",
    "\n",
    "Now we will make this process manual for the unsupported llm endpoint.\n",
    "\n",
    "To make this we will use `langchain.tools`.\n",
    "Just decorating the function we can get format that can use in prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This converted tool has attributes like below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add\n",
      "Add two integers.\n",
      "\n",
      "    Args:\n",
      "        a: First integer\n",
      "        b: Second integer\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n",
      "multiply\n",
      "Multiply two integers.\n",
      "\n",
      "    Args:\n",
      "        a: First integer\n",
      "        b: Second integer\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "langchain_tools = [add, multiply]\n",
    "\n",
    "for tool in langchain_tools:\n",
    "    print(tool.name)\n",
    "    print(tool.description)\n",
    "    print(tool.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render tools to use in prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add(a: int, b: int) -> int - Add two integers.\n",
      "\n",
      "    Args:\n",
      "        a: First integer\n",
      "        b: Second integer\n",
      "multiply(a: int, b: int) -> int - Multiply two integers.\n",
      "\n",
      "    Args:\n",
      "        a: First integer\n",
      "        b: Second integer\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import render_text_description\n",
    "\n",
    "\n",
    "rendered_tools = render_text_description(langchain_tools)\n",
    "tool_names = [tool.name for tool in langchain_tools]\n",
    "print(rendered_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is prompt that we will use.\n",
    "\n",
    "Prompt generates a json format with key, name and args.\n",
    "`name` key is name of the tool we will use, but if there is not proper tool to call it will generate refusal.\n",
    "We will use this refusal later, to handle edge case.\n",
    "And `args` are dict type that contains values which is used to invoke tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# https://smith.langchain.com/hub/hwchase17/react-chat-json\n",
    "system_prompt = \"\"\"\\\n",
    "TOOLS\n",
    "------\n",
    "Assistant can ask the user to use tools to look up information that may be helpful in answering the users original question.\n",
    "The tools the human can use are:\n",
    "\n",
    "{rendered_tools}\n",
    "\n",
    "RESPONSE FORMAT INSTRUCTIONS\n",
    "----------------------------\n",
    "\n",
    "When responding to me, please output a response in one of three formats:\n",
    "\n",
    "**Option 1:**\n",
    "Use this if you cannot determine which tool to use.\n",
    "Markdown code snippet formatted in the following schema:\n",
    "\n",
    "{{\n",
    "    \"name\": \"refusal\"\n",
    "}}\n",
    "\n",
    "\n",
    "**Option 2:**\n",
    "Use this if you want the human to use a tool.\n",
    "Markdown code snippet formatted in the following schema:\n",
    "\n",
    "{{\n",
    "    \"name\": string, \\\\ The action to take. Must be one of {tool_names}\n",
    "    \"args\": dict, \\\\ The input to the action described {rendered_tools}\n",
    "}}\n",
    "\n",
    "USER'S INPUT\n",
    "--------------------\n",
    "\n",
    "Here is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt), (\"user\", \"{input}\")]\n",
    ")\n",
    "prompt = prompt.partial(\n",
    "    rendered_tools=rendered_tools,\n",
    "    tool_names=tool_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will return with json type.\n",
    "So we can easily parse this with `JsonOutputParser`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "{\n",
      "    \"name\": \"multiply\",\n",
      "    \"args\": {\n",
      "        \"a\": 3,\n",
      "        \"b\": 12\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | hug_llm\n",
    "response = chain.invoke({\"What is 3 * 12?\"})\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'multiply', 'args': {'a': 3, 'b': 12}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "chain = prompt | hug_llm | JsonOutputParser()\n",
    "response = chain.invoke({\"What is 3 * 12?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With [langchain tutorial](https://python.langchain.com/docs/how_to/tools_prompting/), we will define a function `invoke_tool` to call the proper tool and give its output to chain result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional, TypedDict\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "class ToolCallRequest(TypedDict):\n",
    "    \"\"\"A typed dict that shows the inputs into the invoke_tool function.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    arguments: Dict[str, Any]\n",
    "\n",
    "\n",
    "def invoke_tool(\n",
    "    tool_call_request: ToolCallRequest, config: Optional[RunnableConfig] = None\n",
    "):\n",
    "    \"\"\"A function that we can use the perform a tool invocation.\n",
    "\n",
    "    Args:\n",
    "        tool_call_request: a dict that contains the keys name and arguments.\n",
    "            The name must match the name of a tool that exists.\n",
    "            The arguments are the arguments to that tool.\n",
    "        config: This is configuration information that LangChain uses that contains\n",
    "            things like callbacks, metadata, etc.See LCEL documentation about RunnableConfig.\n",
    "\n",
    "    Returns:\n",
    "        output from the requested tool\n",
    "    \"\"\"\n",
    "    if tool_call_request[\"name\"] == \"refusal\":\n",
    "        return None\n",
    "    tool_name_to_tool = {tool.name: tool for tool in langchain_tools}\n",
    "    name = tool_call_request[\"name\"]\n",
    "    requested_tool = tool_name_to_tool[name]\n",
    "    return requested_tool.invoke(tool_call_request[\"args\"], config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function can use like below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke_tool({\"name\": \"multiply\", \"args\": {\"a\": 3, \"b\": 12}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make a chain to use `invoke_tool` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | hug_llm | JsonOutputParser() | invoke_tool\n",
    "chain.invoke({\"input\": \"What is 3 * 12?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this as a chain we can use `RunnablePassthrough` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'output': 36}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    prompt\n",
    "    | hug_llm\n",
    "    | JsonOutputParser()\n",
    "    | RunnablePassthrough.assign(output=invoke_tool)\n",
    ")\n",
    "chain.invoke({\"input\": \"What is 3 * 12?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't make our prompt to return multiple tool calling, so we cannot use multiple tools here.\n",
    "\n",
    "And this is why functions like `JsonOutputToolsParser` has an argument like `first_tool_only`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'output': 36}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is 3 * 12? Also, what is 11 + 49?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can see that our prompt is using only first tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'add', 'args': {'a': 11, 'b': 49}, 'output': 60}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is 11 + 49? Also, what is 3 * 12?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a case with error handling like not proper tool to call:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'refusal', 'output': None}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Tools\n",
    "\n",
    "Now we will fix prompt little bit to support multiple tool calling.\n",
    "Return values with list and add Option 3 for multiple tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# https://smith.langchain.com/hub/hwchase17/react-chat-json\n",
    "system_prompt = \"\"\"\n",
    "TOOLS\n",
    "------\n",
    "Assistant can ask the user to use tools to look up information that may be helpful in answering the users original question.\n",
    "The tools the human can use are:\n",
    "\n",
    "{rendered_tools}\n",
    "\n",
    "RESPONSE FORMAT INSTRUCTIONS\n",
    "----------------------------\n",
    "\n",
    "When responding to me, please output a response in one of three formats:\n",
    "\n",
    "**Option 1:**\n",
    "Use this if you cannot determine which tool to use.\n",
    "Markdown code snippet formatted in the following schema:\n",
    "\n",
    "[\n",
    "    {{\n",
    "        \"name\": \"refusal\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "\n",
    "**Option 2:**\n",
    "Use this if you want the human to use a tool.\n",
    "Markdown code snippet formatted in the following schema:\n",
    "\n",
    "[\n",
    "    {{\n",
    "        \"name\": string, \\\\ The action to take. Must be one of {tool_names}\n",
    "        \"args\": dict, \\\\ The input to the action described {rendered_tools}\n",
    "    }}\n",
    "]\n",
    "\n",
    "**Option 3:**\n",
    "Use this if you want the human to use multiple tools.\n",
    "Markdown code snippet formatted in the following schema:\n",
    "\n",
    "[\n",
    "    {{\n",
    "        \"name\": string, \\\\ The action to take. Must be one of {tool_names}\n",
    "        \"args\": dict, \\\\ The input to the action described {rendered_tools}\n",
    "    }},\n",
    "    {{\n",
    "        \"name\": string, \\\\ The action to take. Must be one of {tool_names}\n",
    "        \"args\": dict, \\\\ The input to the action described {rendered_tools}\n",
    "    }}\n",
    "]\n",
    "\n",
    "USER'S INPUT\n",
    "--------------------\n",
    "\n",
    "Here is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt), (\"user\", \"{input}\")]\n",
    ")\n",
    "prompt = prompt.partial(\n",
    "    rendered_tools=rendered_tools,\n",
    "    tool_names=tool_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it will return like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply', 'args': {'a': 3, 'b': 12}}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "chain = prompt | hug_llm | JsonOutputParser()\n",
    "response = chain.invoke({\"What is 3 * 12?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this we have to fix the schema.\n",
    "\n",
    "First write `ToolCall` which is same as before example.\n",
    "And write `ToolCallRequests` which contains list of `ToolCall`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "def invoke_tools(\n",
    "    tool_call_requests: Dict[Any, Any], config: Optional[RunnableConfig] = None\n",
    "):\n",
    "    \"\"\"A function that we can use the perform a tool invocation.\n",
    "\n",
    "    Args:\n",
    "        tool_call_requests: a list with dicts and dict contains the keys name and arguments.\n",
    "            The name must match the name of a tool that exists.\n",
    "            The arguments are the arguments to that tool.\n",
    "        config: This is configuration information that LangChain uses that contains\n",
    "            things like callbacks, metadata, etc.See LCEL documentation about RunnableConfig.\n",
    "\n",
    "    Returns:\n",
    "        output from the requested tool\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for tool_call_request in tool_call_requests[\"tool_calls\"]:\n",
    "        if tool_call_request[\"name\"] == \"refusal\":\n",
    "            return None\n",
    "\n",
    "        tool_name_to_tool = {tool.name: tool for tool in langchain_tools}\n",
    "        name = tool_call_request[\"name\"]\n",
    "        requested_tool = tool_name_to_tool[name]\n",
    "        response = requested_tool.invoke(tool_call_request[\"args\"], config=config)\n",
    "        results += [response]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnablePassthrough` function needs input as a dictionary.\n",
    "So we will add a dictionary to the parser.\n",
    "You can see that output has been changed as list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_calls': [{'name': 'multiply', 'args': {'a': 3, 'b': 12}}],\n",
       " 'output': [36]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    prompt\n",
    "    | hug_llm\n",
    "    | {\"tool_calls\": JsonOutputParser()}\n",
    "    | RunnablePassthrough.assign(output=invoke_tools)\n",
    ")\n",
    "chain.invoke({\"input\": \"What is 3 * 12?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can handle multiple tool calling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_calls': [{'name': 'multiply', 'args': {'a': 3, 'b': 12}},\n",
       "  {'name': 'add', 'args': {'a': 11, 'b': 49}}],\n",
       " 'output': [36, 60]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is 3 * 12? Also, what is 11 + 49?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also order is preserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_calls': [{'name': 'add', 'args': {'a': 11, 'b': 49}},\n",
       "  {'name': 'multiply', 'args': {'a': 3, 'b': 12}}],\n",
       " 'output': [60, 36]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is 11 + 49? Also, what is 3 * 12?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a case with error handling like not proper tool to call:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_calls': [{'name': 'refusal'}], 'output': None}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thoughts\n",
    "\n",
    "Now lets use the tool output to generate answer with user query.\n",
    "To generate answer we have to keep all messages on the chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'multiply', 'args': {'a': 3, 'b': 12}}, {'name': 'add', 'args': {'a': 11, 'b': 49}}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
    "\n",
    "\n",
    "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
    "\n",
    "chain = prompt | hug_llm | {\"tool_calls\": JsonOutputParser()}\n",
    "\n",
    "messages = [HumanMessage(query)]\n",
    "ai_msg = chain.invoke(messages)\n",
    "print(ai_msg[\"tool_calls\"])\n",
    "messages.append(AIMessage(str(ai_msg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack tool results to the message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 3 * 12? Also, what is 11 + 49?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"{'tool_calls': [{'name': 'multiply', 'args': {'a': 3, 'b': 12}}, {'name': 'add', 'args': {'a': 11, 'b': 49}}]}\", additional_kwargs={}, response_metadata={}),\n",
       " ToolMessage(content='36', name='multiply', tool_call_id='tmp-multiply-36'),\n",
       " ToolMessage(content='60', name='add', tool_call_id='tmp-add-60')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for tool_call in ai_msg[\"tool_calls\"]:\n",
    "    name = tool_call[\"name\"].lower()\n",
    "    selected_tool = {\"add\": add, \"multiply\": multiply}[name]\n",
    "    tool_msg = selected_tool.invoke(tool_call[\"args\"])\n",
    "    messages.append(\n",
    "        ToolMessage(content=tool_msg, name=name, tool_call_id=f\"tmp-{name}-{tool_msg}\")\n",
    "    )\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can ask llm to answer with this results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "3 * 12 equals 36.\n",
      "\n",
      "11 + 49 equals 60.\n",
      "\n",
      "So, the results are:\n",
      "- 3 * 12 = 36\n",
      "- 11 + 49 = 60\n",
      "\n",
      "In terms of the tool calls:\n",
      "- The 'multiply' tool call with arguments 3 and 12 gives the result 36.\n",
      "- The 'add' tool call with arguments\n"
     ]
    }
   ],
   "source": [
    "hug_llm.invoke(messages).pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-from-scratch-3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
