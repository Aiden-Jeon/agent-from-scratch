{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "env_file = \"../.env\"\n",
    "if os.path.exists(env_file):\n",
    "    load_dotenv()\n",
    "else:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"<YOUR-OPENAI-API-KEY>\"\n",
    "    os.environ[\"HF_TOKEN\"] = \"<YOUR-HUGGINGFACE-API-KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I am a language model created by OpenAI called GPT-3.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatLiteLLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "# openai_llm = ChatLiteLLM(model=\"openai/gpt-3.5-turbo\")\n",
    "openai_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "messages = [HumanMessage(content=\"what model are you\")]\n",
    "openai_llm.invoke(messages).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Iâ€™m a large language model. When you ask me a question or provide me with a prompt, I analyze what you say and generate a response that is relevant and accurate. I'm constantly learning and improving, so over time I'll be even better at assisting you. Is there anything I can help you with?\n"
     ]
    }
   ],
   "source": [
    "hug_llm = ChatLiteLLM(\n",
    "    model=\"huggingface/meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    temperature=0.2,\n",
    ")\n",
    "messages = [HumanMessage(content=\"what model are you\")]\n",
    "hug_llm.invoke(messages).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Output\n",
    "\n",
    "In this tutorial we will see how the structured output is working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain\n",
    "\n",
    "In langchain you can use `with_structured_output` attribute to get structured output.\n",
    "\n",
    "- https://python.langchain.com/docs/how_to/structured_output/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Pydantic\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: Optional[int] = Field(\n",
    "        default=None, description=\"How funny the joke is, from 1 to 10\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `Joke` structure with pydantic format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo\tsetup='Why was the cat sitting on the computer?' punchline='To keep an eye on the mouse!' rating=8\n",
      "gpt-4o-mini\tsetup='Why was the cat sitting on the computer?' punchline='Because it wanted to keep an eye on the mouse!' rating=7\n"
     ]
    }
   ],
   "source": [
    "# In OpenAI\n",
    "# both supports\n",
    "for model_name in [\"gpt-3.5-turbo\", \"gpt-4o-mini\"]:\n",
    "    temp_llm = ChatOpenAI(model=model_name)\n",
    "    structured_llm = temp_llm.with_structured_output(Joke)\n",
    "    response = structured_llm.invoke(\"Tell me a joke about cats\")\n",
    "    print(model_name, response, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check that both `gpt-3.5-turbo` and `gpt-4o-mini` are generating output with given format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI\n",
    "\n",
    "But in [OpenAI document](https://platform.openai.com/docs/guides/structured-outputs), only after gpt-4 models are possible to use structure output format.\n",
    "\n",
    "> Structured Outputs is the evolution of JSON mode. While both ensure valid JSON is produced, only Structured Outputs ensure schema adherance. Both Structured Outputs and JSON mode are supported in the Chat Completions API, Assistants API, Fine-tuning API and Batch API.\n",
    "> \n",
    "> |                        | Structured Outputs                                           | JSON Mode                                        |\n",
    "> | :--------------------- | :----------------------------------------------------------- | :----------------------------------------------- |\n",
    "> | **Outputs valid JSON** | Yes                                                          | Yes                                              |\n",
    "> | **Adheres to schema**  | Yes (see [supported schemas](https://platform.openai.com/docs/guides/structured-outputs#supported-schemas)) | No                                               |\n",
    "> | **Compatible models**  | `gpt-4o-mini`, `gpt-4o-2024-08-06`, and later                | `gpt-3.5-turbo`, `gpt-4-*` and `gpt-4o-*` models |\n",
    "> | **Enabling**           | `response_format: { type: \"json_schema\", json_schema: {\"strict\": true, \"schema\": ...} }` | `response_format: { type: \"json_object\" }`       |\n",
    "\n",
    "\n",
    "This is why we use langchain, because it made a unified usage for the various model.\n",
    "But sometimes there are llm endpoints that cannot use this usage, so its easy usage makes users confused.\n",
    "We will see whats going on behind the langchain framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT-4o-mini ~\n",
    "\n",
    "After the `gpt-4-*` models support directly using structured output, so its usage is same as langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just use `response_format` with pydantic class.\n",
    "It will return the structured output as we saw earlier.\n",
    "This response_format guarantee format for every generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup='Why was the cat sitting on the computer?' punchline='Because it wanted to keep an eye on the mouse!' rating=7\n",
      "setup='Why was the cat sitting on the computer?' punchline='Because it wanted to keep an eye on the mouse!' rating=7\n",
      "setup='Why was the cat sitting on the computer?' punchline='Because it wanted to keep an eye on the mouse!' rating=7\n",
      "setup='Why was the cat sitting on the computer?' punchline='Because it wanted to keep an eye on the mouse!' rating=7\n",
      "setup='Why did the cat sit on the computer?' punchline='Because it wanted to keep an eye on the mouse!' rating=7\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Tell me a joke about cats\",\n",
    "            },\n",
    "        ],\n",
    "        response_format=Joke,\n",
    "    )\n",
    "    event = completion.choices[0].message.parsed\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ~ GPT-3.5-turbo\n",
    "\n",
    "But old models before `gpt-3.5-*`, we can not use pydantic format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"Invalid parameter: 'response_format' of type 'json_schema' is not supported with this model. Learn more about supported models at the Structured Outputs guide: https://platform.openai.com/docs/guides/structured-outputs\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Tell me a joke about cats\",\n",
    "            },\n",
    "        ],\n",
    "        response_format=Joke,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to give json parsing instruction.\n",
    "We can use both way to use this:\n",
    "- Tool Calling\n",
    "- Json Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tool calling\n",
    "\n",
    "OpenAI endpoint supports tool calling parameters.\n",
    "For the base tool usage diagram is like this:\n",
    "\n",
    "<img src=\"../assets/function-calling-diagram.png\" width=50%>\n",
    "\n",
    "\n",
    "OpenAI don't use directly tools.\n",
    "It just response to client, that llm need to use some tools to generate proper response.\n",
    "\n",
    "But some functions are supported that llm use directly.\n",
    "It's representative example is pydantic functions tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'Joke',\n",
       "  'strict': True,\n",
       "  'parameters': {'description': 'Joke to tell user.',\n",
       "   'properties': {'setup': {'description': 'The setup of the joke',\n",
       "     'title': 'Setup',\n",
       "     'type': 'string'},\n",
       "    'punchline': {'description': 'The punchline to the joke',\n",
       "     'title': 'Punchline',\n",
       "     'type': 'string'},\n",
       "    'rating': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "     'description': 'How funny the joke is, from 1 to 10',\n",
       "     'title': 'Rating'}},\n",
       "   'required': ['setup', 'punchline', 'rating'],\n",
       "   'title': 'Joke',\n",
       "   'type': 'object',\n",
       "   'additionalProperties': False},\n",
       "  'description': 'Joke to tell user.'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai.pydantic_function_tool(Joke)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `openai.pydantic_function_tool` generates json with OpenAI function calling format.\n",
    "And for this functions it is directly used in OpenAI's LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"setup\":\"Why was the cat sitting on the computer?\",\"punchline\":\"He wanted to keep an eye on the mouse!\",\"rating\":8}\n",
      "{\"setup\":\"Why don't cats play poker in the jungle?\",\"punchline\":\"Too many cheetahs!\",\"rating\":7}\n",
      "{\"setup\":\"Why was the cat sitting on the computer?\",\"punchline\":\"He wanted to keep an eye on the mouse!\",\"rating\":8}\n",
      "{\"setup\":\"Why was the cat sitting on the computer?\",\"punchline\":\"It wanted to keep an eye on the mouse!\",\"rating\":8}\n",
      "{\"setup\":\"Why was the cat sitting on the computer?\",\"punchline\":\"He wanted to keep an eye on the mouse!\",\"rating\":8}\n"
     ]
    }
   ],
   "source": [
    "tools = [openai.pydantic_function_tool(Joke)]\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Tell me a joke about cats\",\n",
    "            },\n",
    "        ],\n",
    "        tools=tools,\n",
    "    )\n",
    "    event = completion.choices[0].message.tool_calls[0].function\n",
    "    print(event.arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this generates json format well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Json mode\n",
    "\n",
    "For the old usage when the function calling was not supported, we have to use `response_format={\"type\": \"json_object\"}`.\n",
    "This is why many old tutorial explains this usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"type\": \"joke\",\n",
      "    \"category\": \"cats\",\n",
      "    \"content\": \"Why was the cat sitting on the computer? Because it wanted to keep an eye on the mouse!\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"Extract the event information.\\nReturn the output as JSON\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me a joke about cats\",\n",
    "        },\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")\n",
    "event = completion.choices[0].message.content\n",
    "print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this response_format we have to write JSON in prompt clearly.\n",
    "Otherwise, you will get error for the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"Extract the event information.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Tell me a joke about cats\",\n",
    "            },\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But without any instruction for the our desired key and output, it will generate a random json format like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"topic\": \"cats\",\n",
      "    \"type\": \"joke\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"Extract the event information.\\nReturn the output as JSON\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me a joke about cats\",\n",
    "        },\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")\n",
    "event = completion.choices[0].message.content\n",
    "print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have to give an instruction for our desired format.\n",
    "Let's use the openai tool for the instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'Joke to tell user.',\n",
       " 'properties': {'setup': {'description': 'The setup of the joke',\n",
       "   'title': 'Setup',\n",
       "   'type': 'string'},\n",
       "  'punchline': {'description': 'The punchline to the joke',\n",
       "   'title': 'Punchline',\n",
       "   'type': 'string'},\n",
       "  'rating': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "   'description': 'How funny the joke is, from 1 to 10',\n",
       "   'title': 'Rating'}},\n",
       " 'required': ['setup', 'punchline', 'rating'],\n",
       " 'title': 'Joke',\n",
       " 'type': 'object',\n",
       " 'additionalProperties': False}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pydantic_parse = openai.pydantic_function_tool(Joke)[\"function\"][\"parameters\"]\n",
    "pydantic_parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this json format is lack of guarantee for the filled value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"setup\": \"Why was the cat sitting on the computer?\",\n",
      "  \"punchline\": \"To keep an eye on the mouse!\",\n",
      "  \"rating\": null\n",
      "}\n",
      "{\n",
      "  \"setup\": \"Why was the cat sitting on the computer?\",\n",
      "  \"punchline\": \"To keep an eye on the mouse!\",\n",
      "  \"rating\": null\n",
      "}\n",
      "{\n",
      "    \"setup\": \"Why was the cat sitting on the computer?\",\n",
      "    \"punchline\": \"To keep an eye on the mouse!\",\n",
      "    \"rating\": 8\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "pydantic_parse = openai.pydantic_function_tool(Joke)[\"function\"][\"parameters\"]\n",
    "\n",
    "for _ in range(3):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"Extract the event information.\\n{pydantic_parse}\\nReturn the output as JSON\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Tell me a joke about cats\",\n",
    "            },\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    event = completion.choices[0].message.content\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Langchain with Json Mode\n",
    "\n",
    "In this case we can use langchain for the output instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"description\": \"Joke to tell user.\", \"properties\": {\"setup\": {\"description\": \"The setup of the joke\", \"title\": \"Setup\", \"type\": \"string\"}, \"punchline\": {\"description\": \"The punchline to the joke\", \"title\": \"Punchline\", \"type\": \"string\"}, \"rating\": {\"anyOf\": [{\"type\": \"integer\"}, {\"type\": \"null\"}], \"default\": null, \"description\": \"How funny the joke is, from 1 to 10\", \"title\": \"Rating\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "parser_instruction = parser.get_format_instructions()\n",
    "print(parser_instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More detail instruction guarantee for the filled output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"setup\": \"Why was the cat sitting on the computer?\",\n",
      "  \"punchline\": \"Because it wanted to keep an eye on the mouse!\",\n",
      "  \"rating\": 7\n",
      "}\n",
      "{\n",
      "  \"setup\": \"Why was the cat sitting on the computer?\",\n",
      "  \"punchline\": \"Because it wanted to keep an eye on the mouse!\",\n",
      "  \"rating\": 7\n",
      "}\n",
      "{\n",
      "  \"setup\": \"Why was the cat sitting on the computer?\",\n",
      "  \"punchline\": \"Because it wanted to keep an eye on the mouse!\",\n",
      "  \"rating\": 7\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "pydantic_parse = openai.pydantic_function_tool(Joke)[\"function\"][\"parameters\"]\n",
    "\n",
    "for _ in range(3):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": parser_instruction,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Tell me a joke about cats\",\n",
    "            },\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    event = completion.choices[0].message.content\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion\n",
    "\n",
    "In my opinion, for the structured output using OpenAI it is easy to use tools instead response format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface\n",
    "\n",
    "But what if tool or response format is not supported like huggingface?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stream', 'temperature', 'max_tokens', 'max_completion_tokens', 'top_p', 'stop', 'n', 'echo']\n"
     ]
    }
   ],
   "source": [
    "from litellm import get_supported_openai_params\n",
    "\n",
    "response = get_supported_openai_params(\"huggingface/meta-llama/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "print(response)\n",
    "# ['stream', 'temperature', 'max_tokens', 'max_completion_tokens', 'top_p', 'stop', 'n', 'echo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This endpoint cannot use `with_structured_output` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "litellm.UnsupportedParamsError: huggingface does not support parameters: {'tools': [{'type': 'function', 'function': {'name': 'Joke', 'description': 'Joke to tell user.', 'parameters': {'properties': {'setup': {'description': 'The setup of the joke', 'type': 'string'}, 'punchline': {'description': 'The punchline to the joke', 'type': 'string'}, 'rating': {'anyOf': [{'type': 'integer'}, {'type': 'null'}], 'default': None, 'description': 'How funny the joke is, from 1 to 10'}}, 'required': ['setup', 'punchline'], 'type': 'object'}}}], 'tool_choice': 'any'}, for model=meta-llama/Llama-3.1-8B-Instruct. To drop these, set `litellm.drop_params=True` or for proxy:\n",
      "\n",
      "`litellm_settings:\n",
      " drop_params: true`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With Huggingface\n",
    "structured_llm = hug_llm.with_structured_output(Joke)\n",
    "try:\n",
    "    structured_llm.invoke(\"Tell me a joke about cats\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have to use instruction to make json output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LiteLLM Completion\n",
    "\n",
    "To make same usage as OpenAI we will use `litellm.completion` function.\n",
    "\n",
    "First, lets use the prompt that is used on the OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"description\": \"Joke to tell user.\", \"properties\": {\"setup\": {\"description\": \"The setup of the joke\", \"title\": \"Setup\", \"type\": \"string\"}, \"punchline\": {\"description\": \"The punchline to the joke\", \"title\": \"Punchline\", \"type\": \"string\"}, \"rating\": {\"anyOf\": [{\"type\": \"integer\"}, {\"type\": \"null\"}], \"default\": null, \"description\": \"How funny the joke is, from 1 to 10\", \"title\": \"Rating\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "parser_instruction = parser.get_format_instructions()\n",
    "print(parser_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "**Setup:** Why did the cat join a band?\n",
      "\n",
      "**Punchline:** Because it wanted to be the purr-cussionist!\n",
      "\n",
      "How's that?\n",
      "Here's one:\n",
      "\n",
      "**Setup:** Why did the cat join a band?\n",
      "\n",
      "**Punchline:** Because it wanted to be the purr-cussionist!\n",
      "\n",
      "How's that?\n",
      "Here's one:\n",
      "\n",
      "**Setup:** Why did the cat join a band?\n",
      "\n",
      "**Punchline:** Because it wanted to be the purr-cussionist!\n",
      "\n",
      "How's that?\n"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    completion = litellm.completion(\n",
    "        model=\"huggingface/meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": parser_instruction,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Tell me a joke about cats\",\n",
    "            },\n",
    "        ],\n",
    "        max_tokens=4048,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "\n",
    "    event = completion.choices[0].message.content\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this is not json format.\n",
    "We have to parse this output as Json."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langchain Output Parser\n",
    "\n",
    "In this case we can use langchain to parse the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a ai assistant.\\nFollow the format and answer \\nFormat: {format_instructions}\",\n",
    "        ),\n",
    "        (\"user\", \"Question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "chain = prompt | hug_llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'setup': 'Why did the cat join a band?', 'punchline': 'Because it wanted to be the purr-cussionist!', 'rating': 8}\n",
      "{'setup': 'Why did the cat join a band?', 'punchline': 'Because it wanted to be the purr-cussionist!', 'rating': 8}\n",
      "{'setup': 'Why did the cat join a band?', 'punchline': 'Because it wanted to be the purr-cussionist!', 'rating': 8}\n",
      "{'setup': 'Why did the cat join a band?', 'punchline': 'Because it wanted to be the purr-cussionist!', 'rating': 8}\n",
      "{'setup': 'Why did the cat join a band?', 'punchline': 'Because it wanted to be the purr-cussionist!', 'rating': 8}\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    response = chain.invoke({\"question\": \"Tell me a joke about cats\"})\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check that output is in json format.\n",
    "But this doesn't mean that this guarantee always generating the answer in json format.\n",
    "\n",
    "In [OpenAI document](https://platform.openai.com/docs/guides/function-calling#edge-cases), it suggest using the json_format like this:\n",
    "\n",
    "```python\n",
    "# Check if the conversation was too long for the context window\n",
    "if response['choices'][0]['message']['finish_reason'] == \"length\":\n",
    "    print(\"Error: The conversation was too long for the context window.\")\n",
    "    # Handle the error as needed, e.g., by truncating the conversation or asking for clarification\n",
    "    handle_length_error(response)\n",
    "\n",
    "# Check if the model's output included copyright material (or similar)\n",
    "if response['choices'][0]['message']['finish_reason'] == \"content_filter\":\n",
    "    print(\"Error: The content was filtered due to policy violations.\")\n",
    "    # Handle the error as needed, e.g., by modifying the request or notifying the user\n",
    "    handle_content_filter_error(response)\n",
    "\n",
    "# Check if the model has made a tool_call. This is the case either if the \"finish_reason\" is \"tool_calls\" or if the \"finish_reason\" is \"stop\" and our API request had forced a function call\n",
    "if (response['choices'][0]['message']['finish_reason'] == \"tool_calls\" or\n",
    "    # This handles the edge case where if we forced the model to call one of our functions, the finish_reason will actually be \"stop\" instead of \"tool_calls\"\n",
    "    (our_api_request_forced_a_tool_call and response['choices'][0]['message']['finish_reason'] == \"stop\")):\n",
    "    # Handle tool call\n",
    "    print(\"Model made a tool call.\")\n",
    "    # Your code to handle tool calls\n",
    "    handle_tool_call(response)\n",
    "\n",
    "# Else finish_reason is \"stop\", in which case the model was just responding directly to the user\n",
    "elif response['choices'][0]['message']['finish_reason'] == \"stop\":\n",
    "    # Handle the normal stop case\n",
    "    print(\"Model responded directly to the user.\")\n",
    "    # Your code to handle normal responses\n",
    "    handle_normal_response(response)\n",
    "\n",
    "# Catch any other case, this is unexpected\n",
    "else:\n",
    "    print(\"Unexpected finish_reason:\", response['choices'][0]['message']['finish_reason'])\n",
    "    # Handle unexpected cases as needed\n",
    "    handle_unexpected_case(response)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-from-scratch-3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
