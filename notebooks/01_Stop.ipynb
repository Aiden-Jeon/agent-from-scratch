{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "env_file = \"../.env\"\n",
    "if os.path.exists(env_file):\n",
    "    load_dotenv()\n",
    "else:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"<YOUR-OPENAI-API-KEY>\"\n",
    "    os.environ[\"HF_TOKEN\"] = \"<YOUR-HUGGINGFACE-API-KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I am a language model developed by OpenAI called GPT-3 (Generative Pre-trained Transformer 3).\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatLiteLLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "# openai_llm = ChatLiteLLM(model=\"openai/gpt-3.5-turbo\")\n",
    "openai_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "messages = [HumanMessage(content=\"what model are you\")]\n",
    "openai_llm.invoke(messages).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Iâ€™m a large language model. When you ask me a question or provide me with a prompt, I analyze what you say and generate a response that is relevant and accurate. I'm constantly learning and improving, so over time I'll be even better at assisting you. Is there anything I can help you with?\n"
     ]
    }
   ],
   "source": [
    "hug_llm = ChatLiteLLM(\n",
    "    model=\"huggingface/meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    temperature=0.2,\n",
    ")\n",
    "messages = [HumanMessage(content=\"what model are you\")]\n",
    "hug_llm.invoke(messages).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supported parameters\n",
    "\n",
    "With `litellm.get_supported_openai_params` function, check the parameters that our llm endpoint can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['frequency_penalty', 'logit_bias', 'logprobs', 'top_logprobs', 'max_tokens', 'max_completion_tokens', 'modalities', 'prediction', 'n', 'presence_penalty', 'seed', 'stop', 'stream', 'stream_options', 'temperature', 'top_p', 'tools', 'tool_choice', 'function_call', 'functions', 'max_retries', 'extra_headers', 'parallel_tool_calls', 'response_format', 'user']\n"
     ]
    }
   ],
   "source": [
    "from litellm import get_supported_openai_params\n",
    "\n",
    "response = get_supported_openai_params(\"gpt-3.5-turbo\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gpt-3.5-turbo` supports various of functions, and in this tutorial series we will check how the tool-related functions are working.\n",
    "- `\"frequency_penalty\"`\n",
    "- `\"logit_bias\"`\n",
    "- `\"logprobs\"`\n",
    "- `\"top_logprobs\"`\n",
    "- `\"max_tokens\"`\n",
    "- `\"max_completion_tokens\"`\n",
    "- `\"modalities\"`\n",
    "- `\"prediction\"`\n",
    "- `\"n\"`\n",
    "- `\"presence_penalty\"`\n",
    "- `\"seed\"`\n",
    "- `\"stop\"`\n",
    "- `\"stream\"`\n",
    "- `\"stream_options\"`\n",
    "- `\"temperature\"`\n",
    "- `\"top_p\"`\n",
    "- `\"tools\"`\n",
    "- `\"tool_choice\"`\n",
    "- `\"function_call\"`\n",
    "- `\"functions\"`\n",
    "- `\"max_retries\"`\n",
    "- `\"extra_headers\"`\n",
    "- `\"parallel_tool_calls\"`\n",
    "- `\"response_format\"`\n",
    "- `\"user\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stream', 'temperature', 'max_tokens', 'max_completion_tokens', 'top_p', 'stop', 'n', 'echo']\n"
     ]
    }
   ],
   "source": [
    "from litellm import get_supported_openai_params\n",
    "\n",
    "response = get_supported_openai_params(\"huggingface/meta-llama/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "print(response)\n",
    "# ['stream', 'temperature', 'max_tokens', 'max_completion_tokens', 'top_p', 'stop', 'n', 'echo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, huggingface endpoint does not support tool-related functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `stop` function\n",
    "\n",
    "In this tutorial we will check stop function.\n",
    "Both OpenAI and Huggingface support about stop function.\n",
    "When given token is generated llm stops generating.\n",
    "\n",
    "To use this `stop` function in langchain, we have to use `bind` attribute.\n",
    "In practice, bind are used to bind structured outputs, tools, functions.\n",
    "These are used with `**kwargs` form and have to define about such items.\n",
    "But `stop` function doesn't require any other.\n",
    "\n",
    "- Reference\n",
    "    - [Langchain document](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI.bind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With out `stop` function llm will generate all the given words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'One two three four five.'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Without bind.\n",
    "chain = openai_llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
    "print(response)\n",
    "# Output is 'One two three four five.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But when we bind stop functions, llm stops generating the words after given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'One two \n"
     ]
    }
   ],
   "source": [
    "# With bind.\n",
    "chain = openai_llm.bind(stop=[\"three\"]) | StrOutputParser()\n",
    "\n",
    "response = chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
    "print(response)\n",
    "# Output is 'One two'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differ from OpenAI, Huggingface stops generating after the given token is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"One two three four five.\"\n"
     ]
    }
   ],
   "source": [
    "# Without bind.\n",
    "chain = hug_llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
    "print(response)\n",
    "# Output is 'One two three four five.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"One two three\n"
     ]
    }
   ],
   "source": [
    "# With bind.\n",
    "chain = hug_llm.bind(stop=[\"three\"]) | StrOutputParser()\n",
    "\n",
    "response = chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
    "print(response)\n",
    "# Output is 'One two'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "runmini-3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
